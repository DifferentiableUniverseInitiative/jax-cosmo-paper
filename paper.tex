%% 
%% Copyright 2007-2019 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

%%\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
\documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{minted}
\usepackage{amssymb}
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{aas_macros}
\usepackage{fontawesome}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{xcolor}
\definecolor{xlinkcolor}{cmyk}{1,1,0,0}
\usepackage[colorlinks = true,
            linkcolor = xlinkcolor,
            urlcolor  = xlinkcolor,
            citecolor = xlinkcolor,
            anchorcolor = xlinkcolor]{hyperref}

\newcommand{\nblink}[1]{\href{https://github.com/DifferentiableUniverseInitiative/jax-cosmo-paper/blob/master/notebooks/#1.ipynb}{\faFileCodeO}}
\newcommand{\github}{\href{https://github.com/DifferentiableUniverseInitiative/jax\_cosmo}{\faGithub}}



\journal{Astronomy and Computing}


%%%% new command (JEC)
\newcommand{\nn}{\nonumber}
\newcommand{\numpyro}{\texttt{NumPyro}}
\newcommand{\bydef}{\overset{def.}{\equiv}}


\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{jax-cosmo: End-to-End Differentiable and GPU Accelerated Cosmology Library}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{jax-cosmo contributors}

\address{}

\begin{abstract}
With jax-cosmo, Fisher forecasts are accurate and instantaneous, inference is lightning-fast, survey optimization is trivial. \github
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text

\section{Introduction}


% \section{JAX: Autograd meets XLA}

\section{Modeling and implementation}

In this section, we describe the cosmological modeling provided by jax-cosmo, and its implementation in JAX.

% Explain how the code is structured, mention some tricks of autodiff if we have some
% Follows a bit the structure of the CCL library

\subsection{Background cosmology}
% Usual stuff


\subsection{Growth of perturbations}
% The interesting point to mention here is the differentiable growth ODE


\subsection{Matter power spectrum}
% Here we describe the EH power spectrum, mention possible extensions with emulators


\subsection{Tracers}
% Here we describe the kernels, like in CCL


\subsection{Angular power-spectra}
% Limber integration, this can be an opportunity to talk about differentiation through integrals as well.


\subsection{Validation against the Core Cosmology Library (CCL)} 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/halofit_pk.png}
    \caption{Comparison of matter power spectrum between CCL and jax-cosmo. \textbf{Example of validation plot} \href{https://github.com/DifferentiableUniverseInitiative/jax_cosmo/blob/master/docs/notebooks/CCL_comparison.ipynb}{\faFileCodeO}}
    \label{fig:halofit_comparison}
\end{figure}



\section{Example of applications}
% illustrates why autodiff is interesting from several points of view

\subsection{DES Y1 setting}
% Here we describe the data and 3x2pt model for DES Y1 
 
In this section, we adopt as a setting a DES Y1 3x2pt-like analysis. 


\subsection{Instantaneous Fisher Forecasts}

\begin{minted}{python}
def likelihood(cosmo):
  # Compute mean and covariance of angular Cls, 
  # for specific probes
  mu, cov = gaussian_cl_covariance_and_mean(cosmo, 
                    ell, probes)
  # Return likelihood value
  return gaussian_log_likelihood(data, mu, cov)

# Compute derivatives of the likelihood with 
# respect to cosmological parameters
g = jax.grad(likelihood)(cosmo)

# Compute Fisher matrix of cosmological 
# parameters
F = - jax.hessian(likelihood)(cosmo)
\end{minted}

% Plot showing error in Jacobian computation by finite diff

% Plot showing error on some parameters

\subsection{Survey Optimization by FoM Maximization}
% Needs to mention the tomo challenge, mention that we can backpropagate through a NN and the cosmology model
% Maybe this can be only mentioned in the discussion.

\subsection{Massive Optimal Compression in 3 Lines}

% Here we can implement MOPED for the compression of 

\section{Gradient-based fast cosmological inference}
%
In the following sections we briefly review the statistical methods used to make some cosmological parameter inference. Namely we have used \textit{Stochastic Variational Inference} (aka SVI), \textit{Hybrid Hamiltonian Monte Carlo} (aka HMC), a variant of HMC knows as \textit{No-U-Turn} (aka NUTS), and finally the use of  \textit{Neutral Reparametrization}.   
%
\subsection{Stochastic Variational Inference}
%
Let us remind briefly what is the \textit{Stochastic Variational Inference} method \citep{2012arXiv1206.7051H, 2017arXiv171105597Z} and show where auto-differentiation helps. If one  writes $p(z)$ the prior, $p(\mathcal{D}|z)$ the likelihood and $p(\mathcal{D})$ the marginal likelihood, then thanks to the  Bayes theorem one gets $p(z|\mathcal{D})=p(z)p(\mathcal{D}|z)/p(\mathcal{D})$ the posterior distribution of a model with latent variables $z$ and a set of observations $\mathcal{D}$. The VI aims to get an approximation of this distribution, ie. $p(z|\mathcal{D}) \approx q(z;\lambda)$, by determining the variational parameters $\lambda$ of a predefined distribution. To do so, one uses the Kullback-Leibler divergence of the two distributions $KL(q(z;\lambda)||p(z|\mathcal{D}))$ leading to the following relation
\begin{align}
\log p(\mathcal{D}) &= \mathtt{ELBO} +  KL(q(z;\lambda)||p(z|\mathcal{D})) \label{eq-ELBO} \\
\mathrm{with} \ \mathtt{ELBO} &\equiv -\mathbb{E}_{q(z;\lambda)}\left[ \log q(z;\lambda)\right] + \mathbb{E}_{q(z;\lambda)}\left[ \log p(z,\mathcal{D}) \right] 
\end{align}
which defines the evidence lower bound (ELBO) that one tends to maximize to get the $\lambda$ values and the  highest lower bound on the logarithm of the marginal probability of the observations $\log p(\mathcal{D})$. So, the optimal variational distribution satisfies
\begin{equation}
q(z;\lambda^\ast) = \underset{q(z;\lambda)}{\mathrm{argmax}}\  \mathtt{ELBO} = 
\underset{\lambda}{\mathrm{argmin}}\ \mathcal{L}(\lambda)
\end{equation}
The function $\mathcal{L}(\lambda)$ is the cost function used in practice. It is composed of two parts:
\begin{equation}
\mathcal{L}(\lambda) = \underbrace{\mathbb{E}_{q(z;\lambda)}\left[ \log q(z;\lambda)\right]}_{guide} - \underbrace{\mathbb{E}_{q(z;\lambda)}\left[ \log p(z,\mathcal{D}) \right]}_{model}
\label{eq-loss-svi-1}
\end{equation}
where the \textit{guide}, in the \numpyro\ word, may be a multi-variate gaussian distribution (MVN) for instance. 

Using the auto-differentation tool, one can use "black-box" guides (aka \textit{automatic differentiation variational inference}). As stated by the authors of \citep{2016arXiv160300788K} ADVI helps specifying a variational family appropriate to the model, computing the corresponding objective
function, taking derivatives, and running a gradient-based or coordinate-ascent optimization. First is defined a bijective differentiable transformation $T$ of the original latent variables $z$ into new variables $\xi$, such $\xi=T(z)$ and $z=T^{-1}(\xi)$. One can develop "AutoGuides" (\numpyro\ terminology) that can be adapted to the user models. Then, the cost function reads
\begin{equation}
\mathcal{L}(\lambda) = \underbrace{\mathbb{E}_{q(\xi;\lambda)}\left[ \log q(\xi;\lambda)\right]}_{guide} - \underbrace{\mathbb{E}_{q(\xi;\lambda)}\left[ \log p(\xi,\mathcal{D}) \right]}_{model}
\label{eq-loss-svi-2}
\end{equation}
with
\begin{equation}
p(\xi,\mathcal{D}) \bydef p(T^{-1}(\xi),\mathcal{D}) |J_{T^{-1}}(\xi)|
\end{equation}
which exhibit the Jacobian of the $T^{-1}$ transformation. The main advantage of this reparametrisation is that the minimization can be performed with no bound constraints. 

Now, one is faced with the evaluation of the expectations during gradient descent. The solution is to use what is called \textit{elliptical standardisation} or \textit{re-parametrization trick} or \textit{coordinate transformation} (see references in \citep{2015arXiv150603431K}). Let us illustrate the trick in the case where $q(\xi; \lambda) = \mathcal{N}(\xi; \mu, \Sigma)$ ($\lambda=(\mu, \Sigma)$). One defines $S_\lambda$ such that $S_\lambda(\xi)=\zeta$, eg. $\zeta = L^{-1}(\xi-\mu)$ where $L$ is the Cholesky decomposition of $\Sigma=LL^T$. The Jacobien of this distribution is 1, so the cost function reads
\begin{multline}
-\mathcal{L}(\lambda) = \underbrace{\mathbb{E}_{\eta\sim \mathcal{N}(0,I)}\left[ \log p(T^{-1}(S_\lambda^{-1}(\zeta)),\mathcal{D}) + \log |J_{T^{-1}}(S_\lambda^{-1}(\zeta))| \right]}_{model} \\ + \underbrace{\mathbb{H}[q(\xi;\lambda)]}_{guide}
\label{eq-loss-svi-3}
\end{multline}
where $\mathbb{H}(q)$ is the Shannon entropy of the gaussian distribution which is independent of the model, so its gradient can be computed once for all and reused in user model. Then,
to get $\nabla_\lambda \mathcal{L}$, the $\nabla$ operator can be put inside the expectation which leads to
\begin{multline}
-\nabla_\lambda\mathcal{L}(\lambda) = \mathbb{E}_{\zeta\sim \mathcal{N}(0,I)}\left\{
\left[ \nabla_z \log p(z,\mathcal{D}) \times \nabla_\xi[T^{-1}(\xi)] \right. \right. \\
+ \left. \left. \nabla_\xi \log|J_{T^{-1}}(\xi)| \right] \times \nabla_\lambda S_\lambda^{-1}(\zeta)
\right\}
+ \nabla_\lambda \mathbb{H}[q(\xi;\lambda)]
\label{eq-loss-svi-4}
\end{multline}
For the use case of the MVN (auto)guide, one gets
\begin{align}
-\nabla_\mu \mathcal{L} &= \mathbb{E}_{\zeta\sim \mathcal{N}(0,I)}\left\{
\nabla_z \log p(z,\mathcal{D}) \times \nabla_\xi[T^{-1}(\xi)]
+ \nabla_\xi \log|J_{T^{-1}}(\xi)|
\right\} \nn \\
-\nabla_L \mathcal{L} &= \mathbb{E}_{\zeta\sim \mathcal{N}(0,I)}\left\{
\left[ \nabla_z \log p(z,\mathcal{D}) \times \nabla_\xi[T^{-1}(\xi)] \right.\right. \nn\\
&\left. \left. \qquad\qquad\qquad + \nabla_\xi \log|J_{T^{-1}}(\xi)| \right] \times \xi^T 
\right\}+ (L^{-1})^T
\end{align}
The expectations used in the above expressions are computed with $\zeta$ i.i.d samples from $\mathcal{N}(0,1)$ distribution which reveals the "S" of SVI.



\subsection{Hamiltonian Monte Carlo}

\subsubsection{Introduction}

% Show Joe's vanilla HMC results against Cobaya
Hamiltonian Monte Carlo (HMC) is an MCMC-type method particularly suited to drawing
samples from high dimensional parameter spaces.  It was introduced in \citep{1987PhLB..195..216D}
and developed extensively since.  See \citet{betancourt} for a full review; we describe
very basic features here.

HMC samples spaces by generating particle tracjectories through the space, using the log-posterior
as the negative potential energy of a particle at each point $q$ in the space.  At each sample a trajectory are initialized
with a random momentum $p$, and then integrated using Hamilton's equations with a numerical integration:
\begin{align}
\frac{\mathrm{d}p}{\mathrm{d}t} &= - \frac{\partial V}{\mathrm{d} q} = \frac{\partial \log{\cal P}}{\mathrm{d} q}\\
\frac{\mathrm{d}q}{\mathrm{d}t} &= + \frac{\partial U}{\mathrm{d} p} = M^{-1} p
\end{align}
where $M$ is a mass matrix which should be set to approximate the covariance of the posterior. This is also
used to set the scale of the random initial velocities.

Formally, the set of $n_\mathrm{dim}$ moment are treated as new parameters, and after 
some number of integration steps a final point in the trajectory is compared to the initial one,
and a Metropolis-Hastings acceptance criterion on the total energy $H = p^T M^{-1} p + \log{\cal P}(q)$ is applied.
If the trajectory is perfectly simulation then this acceptance is unity, since energy is conserved; applying it allows
a relaxation to the integration accuracy.

That gradients $\partial \log{\cal P} / \mathrm{d} q$ can be estimated using finite differencing,
but this typically requires at least $4 n_{\mathrm{dim}} + 1$ posterior evaluations per point, greatly slowing it
in high dimension, and as with the Fisher forecasting is highly prone to numerical error. Automatically
calculating a Hessian, as in JAX-Cosmo, makes it feasible and efficient.

Below we show an example set of HMC constraints using JAX-Cosmo to constrain a simulated
Dark Energy Survey Year One (DES-Y1) lensing and clustering-like (3x2pt) likelihood, using 21 parameters\footnote{We omit the 
five clustering sample photometric redshift parameters}.  We re-parametrize the space to unit mass using the Cholesky 
decomposition
of the covariance as estimated with an initial Fisher matrix; this lets us reflect the trajectories when they reach the
edge of the parameter space, as recommended in \citep{NIPS2015_8303a79b}.  We use a fixed number (25) of integration steps and 
a fixed integration period (0.02), this was chosen heuristically to give a reasonable acceptance rate, but is likely to be
far from optimal.  We compare to the highly-optimized {\sc Cobaya} 
Metropolis-Hastings implementation \citep{cobaya2,cobaya1}, initializing it with the Fisher matrix as a proposal and
tuning thereafter.  We ran two processes for each sampler, with four CPU threads per process.

The Hamiltonian sampler required around half the number of samples as the Metropolis (70,000 vs 140,000) to reach approximately the same
convergence, as tested by the Gelman-Rubin test \citep{gelman}, though each sample was itself a few times slower
on a CPU.  This proof-of-principle example can certainly be hugely improved using various extensions to HMC such as No U-Turn Sampling \citep{nuts} or sampling from the entire trajectories \cite{betancourt}.  On next generation GPU high-performance systems, the advantage will be large.

\begin{figure}
\includegraphics[width=0.95\columnwidth]{figures/hmc.pdf}
\caption{Constraints on three of the 21 parameters of a simulated DES-Y1 3x2pt likelihood, using a simple Hamiltonian
Monte Carlo sampler with JAX-Cosmo (blue), compared to the Cobaya Metropolis-Hastings sampler (orange).}
\end{figure}


We note that a full Boltzmann code accurate enough to calculate the CMB spectra to Planck \citep{planck18}
or future \citep{s4,simons} accuracy is currently beyond the scope of this project.  
If this is needed then HMC could use a fast approximation to a CMB theory spectrum, and importance sampling applied to the complete chain.

\subsubsection{NUTS}
%
% Show the advantage of using NUTS 
% look at difference in efficiency in terms of how many times we need to call the model.

A variant of the traditional HMC sampler has been introduced in reference \citep{nuts}. It aims to 


\subsubsection{Neural Reparametrisation}
If the SVI method can be used as is to get $z$ i.i.d. samples from the $q(z,\lambda^\ast)$ distribution, the \textit{Neural Transport MCMC} method \citep{2014arXiv1412.5492P,2019arXiv190303704H} is an efficient way to boost HMC efficiency, especially in target distribution unfavourable geometry where for instance the leapfrog integration algorithm has to face squeezed join distributions for a subset of variables. From SVI, one obtains a first approximation of the target distribution and we show that the change-of-variable $z=F_\lambda(\zeta)$ is such that
\begin{equation}
q(z;\lambda) \rightarrow q(\zeta;\lambda) \bydef q(F_\lambda(\zeta)) |J_{F_\lambda}(\zeta)|
\end{equation}
which is evaluated with optimal $\lambda^\ast$ according to maximization of the ELBO (eg. $F_\lambda=T^{-1}\circ S^{-1}_\lambda$). So, one can use a HMC sampler (eg. NUTS) based on $p(\zeta;\mathcal{D})$ distribution, initialized with $\zeta$ samples from $q(\zeta;\lambda^\ast)$, to get a Markov Chain of $N$ samples $(\zeta_i)_{i<N}$. Then, from the transformation  $z_i=F_{\lambda^\ast}(\zeta_i)$ one finally obtain a Marcov Chain with $(z_i)_{i<N}$ samples. The method is accessible throw the \verb|NeuTraReparam| in \numpyro\ library.


\subsection{Discussion}


\section{Conclusion}


\subsection{Efficient sampling in high dimensions with Hamiltonian Monte Carlo}



\subsection{Lightning fast inference with BayesFast}

%
%
\subsubsection{DES-Y1 Example}
%
Using the same DES-Y1 simulation as in previous section, we have used the \numpyro\ library to first use a simple Multivariate Normal distribution as \textit{guide} to approximate the true posterior (\textit{model}) to feed a SVI. The Adam optimizer has been used with a learning rate set to $10^{-3}$.  Then, we have used the Neural Transport method to generate Markov Chains of different sizes by the NUTS sampler. The SVI optimization requires 20,000 steps and has been performed in about 2 hours on a V100 GPU. Concerning the NUTS running on the same GPU, the chains have run 200 warm samples, and then 200 samples, 1000 samples on a single chain, and finally we have run 10 chains (vectorized) with 1000 samples each. The elapse time for each of these 3 runs is 50 minutes, 150 minutes and 5 hours, respectively. Notice that the \texttt{max\_tree\_depth} parameter has been set to 5 instead of the default value of 10 to speed up running. The results of the evolution of the contours compared to the direct NUTS sampling lasting 20 hours on the same GPU are shown on the figures \ref{fig_svi_nuts_1}, \ref{fig_svi_nuts_2}, \ref{fig_svi_nuts_3}. The  agreement is also rather good for the highly correlated $(b_i)_{i:1,\dots,5}$ bias parameters as show on figure \ref{fig_svi_nuts_4}. Results upon ESS are shown on Tab.~\ref{tab-ESS-NUTS_SVI-1}. On can see that the NUTS sampling with a \texttt{max\_tree\_depth} parameter of 5 after the SVI and Neutral Parametrisation gives same order of efficiencies (lower by a factor 2 for some parameters) compared to the NUTS sampling with block mass matrix and \texttt{max\_tree\_depth} parameter of 7.

The remarkable outcome is that within few hours on a single GPU, one can figure out progressively if the contours have a meaningful appearance before launching multi-GPU multi-chains NUTS sampling to get high statistics Markov Chain with high effective sample size (ESS). 

\begin{figure}
\includegraphics[width=\columnwidth]{figures/SVI_NUTS_1x200x200.pdf}
\caption{Constraints (30\%, 68\%, 90\%) on 4 of the 22 parameters of a simulated DES-Y1 3x2pt likelihood, in blue the NUTS sampling described in..., and in orange a SVI MVN optimisation followed by a Naural Transport parametrisation to run a NUTS sampling (1 chain of 200 warm followed by 200 samples; a dense mass matrix and \texttt{max\_tree\_depth} parameter set to 5).}
\label{fig_svi_nuts_1}
\end{figure}

\begin{figure}
\includegraphics[width=\columnwidth]{figures/SVI_NUTS_1x200x1000.pdf}
\caption{Same as Fig.~\ref{fig_svi_nuts_1} but with 1 chain of 200 warm followed by 1000 samples for the NUTS sampler after SVI optimization.}
\label{fig_svi_nuts_2}
\end{figure}

\begin{figure}
\includegraphics[width=\columnwidth]{figures/SVI_NUTS_10x200x1000.pdf}
\caption{Same as Fig.~\ref{fig_svi_nuts_1} but with 10 chains of 200 warm followed by 1000 samples for the NUTS sampler after SVI optimization.}
\label{fig_svi_nuts_3}
\end{figure}

\begin{figure}
\includegraphics[width=\columnwidth]{figures/SVI_NUTS_1x200x200_bis.pdf}
\caption{In the same conditions of Fig.~\ref{fig_svi_nuts_1} but for the five lens galaxy bias which are highly correlated.}
\label{fig_svi_nuts_4}
\end{figure}


\begin{table}[htb]
\caption{Results of the relative effective sample size (aka ESS, "bulk") in percent computed by the \texttt{Arviz} library \citep{arviz_2019}. (1) are computed from 10 parallel chains of 200 warm up samples followed by 1000 samples each obtained by NUTS sampler after the several uniform distributions (Tab.~\ref{tab-DESY1}) transformed in an homogeneous uniform distribution in the $[-5,5]$ interval, while (2) are the results of SVI followed by NUTS sampler after a Neural Transform  with 10 chains of 200 warm up samples followed by 1000 samples each. In method (1) the \texttt{max\_tree\_depth} parameter is set to 7 while it is set to 5 in the second method.}
\label{tab-ESS-NUTS_SVI-1}
 \centering
\begin{tabular}{ccccccccccc}
\hline
         & $\Omega_b$ & $\Omega_c$ & $\sigma_8$ & $w_0$ & $h$ & $n_s$ & $A$ & $\eta$\\
\hline
(1) & $48.1$ &  $45.6$     & $36.2$     & $33.4$ & $52.8$ & $50.1$ & $68.8$ & $48.8$\\
(2) & $38.7$ &  $24.8$     & $16.0$     & $18.6$ & $43.3$ & $39.5$ & $21.7$ & $16.6$\\
\hline
\end{tabular}
\end{table}



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

\section{Description of the DES-Y1}
\label{app-DESY1}
\begin{table}[htb]
\caption{Priors of the 22 variables of the DES-Y1 of the 3x2pt likelihood (number counts and shear).}
\label{tab-DESY1}
 \centering
\begin{tabular}{ccccccccccc}
\hline
 parameter &  prior \\
 \hline
  \multicolumn{2}{c}{\textbf{Cosmology}} \\
$\Omega_b$ & $\mathcal{U}[0.10, 0.9]$ \\
$\Omega_c$ & $\mathcal{U}[0.40, 1.0]$ \\
$\sigma_8$ & $\mathcal{U}[0.03, 0.07]$ \\
$w_0$ & $\mathcal{U}[-2.00, -0.33]$ \\
$h$ & $\mathcal{U}[0.55, 0.91]$ \\
$n_s$ & $\mathcal{U}[0.87, 1.07]$ \\
\multicolumn{2}{c}{\textbf{Intrinsic Alignment}} \\
$A$ & $\mathcal{U}[-5,5]$ \\
$\eta$ &$\mathcal{U}[-5,5]$ \\
\multicolumn{2}{c}{\textbf{Lens Galaxy Bias}} \\
$(b_i)_{i:1,\dots,5}$ & $\mathcal{U}[0.8,3.0]$ \\
\multicolumn{2}{c}{\textbf{Shear Calibration Systematics}} \\
$(m_i)_{i:1,\dots,4}$ & $\mathcal{N}[0.012,0.023]$ \\
\multicolumn{2}{c}{\textbf{Source photo-$z$ shift}} \\
$dz_1$ & $\mathcal{N}[0.001,0.016]$ \\
$dz_2$ & $\mathcal{N}[-0.019,0.013]$ \\
$dz_3$ & $\mathcal{N}[0.009,0.011]$ \\
$dz_4$ & $\mathcal{N}[-0.018,0.022]$ \\
\hline
\end{tabular}
\end{table}



%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\section*{Bibliography}
\bibliographystyle{elsarticle-harv} 
\bibliography{refs}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.